{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6aGeaMsW+F5AR5/HZ22b2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinayD2028/BigData-Inclass-Assignment/blob/main/BigDataInclassAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AurzSdaFrzY",
        "outputId": "0d94d3e6-2c4f-40b6-9a55-f147d70e35b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLlib tasks complete. Output files saved to 'outputs' directory.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline  # Import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, ChiSqSelector\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "import pandas as pd  # Import pandas for .toPandas()\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CustomerChurnMLlib\").getOrCreate()\n",
        "\n",
        "# Load dataset - CHANGED to new file\n",
        "data_path = \"churn_streaming_data.csv\"\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "# CHANGED to drop new customer_id column\n",
        "df = df.drop(\"customer_id\")\n",
        "\n",
        "# Task 1: Data Preprocessing and Feature Engineering\n",
        "def preprocess_data(df):\n",
        "    # CHANGED: Define new categorical and numerical columns based on churn_streaming_data.csv\n",
        "    categorical_cols = ['region', 'plan_type']\n",
        "    numeric_cols = [\n",
        "        'age', 'monthly_fee', 'tenure_months', 'logins_per_week',\n",
        "        'avg_session_minutes', 'content_watched_per_week',\n",
        "        'num_support_tickets', 'satisfaction_score', 'used_discount'\n",
        "    ]\n",
        "\n",
        "    # Index and encode categorical features\n",
        "    indexers = [\n",
        "        StringIndexer(inputCol=col, outputCol=col + \"_Index\", handleInvalid=\"keep\")\n",
        "        for col in categorical_cols\n",
        "    ]\n",
        "    # Use dropLast=False to make feature selection mapping easier to interpret\n",
        "    encoders = [\n",
        "        OneHotEncoder(inputCol=col + \"_Index\", outputCol=col + \"_Vec\", dropLast=False)\n",
        "        for col in categorical_cols\n",
        "    ]\n",
        "\n",
        "    # Assemble features\n",
        "    feature_cols = [col + \"_Vec\" for col in categorical_cols] + numeric_cols\n",
        "    # --- FIX: Added handleInvalid=\"skip\" ---\n",
        "    # The error log shows VectorAssembler fails on null values.\n",
        "    # \"skip\" will filter out rows that contain nulls in any of the feature columns\n",
        "    # before they are passed to the model.\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "\n",
        "    # --- REVISED: Use a Pipeline for preprocessing ---\n",
        "    # The original code fit and transformed stages individually, which is less efficient\n",
        "    # and can lead to errors. A Pipeline is the correct approach.\n",
        "    pipeline_stages = indexers + encoders + [assembler]\n",
        "    pipeline = Pipeline(stages=pipeline_stages)\n",
        "\n",
        "    pipeline_model = pipeline.fit(df)\n",
        "    df = pipeline_model.transform(df)\n",
        "    # --- End of Pipeline revision ---\n",
        "\n",
        "    # Get metadata from the features column to robustly get feature names later\n",
        "    features_metadata = df.schema[\"features\"].metadata\n",
        "\n",
        "    # Save output sample\n",
        "    with open(f\"{output_dir}/task1_preprocessing_summary.txt\", \"w\") as f:\n",
        "        f.write(\"Task 1: Data Preprocessing and Feature Engineering\\n\")\n",
        "        f.write(f\"Categorical columns: {categorical_cols}\\n\")\n",
        "        f.write(f\"Numerical columns: {numeric_cols}\\n\")\n",
        "        f.write(\"Sample Output:\\n\")\n",
        "        # CHANGED: Label column is now 'churn_flag'\n",
        "        sample = df.select(\"features\", \"churn_flag\").limit(5).toPandas()\n",
        "        f.write(sample.to_string(index=False))\n",
        "\n",
        "    # CHANGED: The label column is 'churn_flag' and is already numeric (0/1)\n",
        "    # No StringIndexer needed for the label.\n",
        "    # We return the metadata for use in Task 3\n",
        "    return df.select(\"features\", \"churn_flag\").withColumnRenamed(\"churn_flag\", \"label\"), features_metadata\n",
        "\n",
        "# Task 2: Train and Evaluate Logistic Regression Model\n",
        "# This function is generic and requires no changes as it uses the \"features\" and \"label\" columns\n",
        "def train_logistic_regression_model(df):\n",
        "    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "    model = lr.fit(train_df)\n",
        "    predictions = model.transform(test_df)\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol=\"label\")\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "\n",
        "    with open(f\"{output_dir}/task2_logistic_regression_results.txt\", \"w\") as f:\n",
        "        f.write(\"Task 2: Logistic Regression Evaluation\\n\")\n",
        "        f.write(f\"Logistic Regression Model Accuracy (AUC): {auc:.2f}\\n\")\n",
        "\n",
        "# Task 3: Feature Selection using Chi-Square\n",
        "# REVISED: This function now uses the metadata to correctly map feature indices to names\n",
        "def feature_selection(df, metadata):\n",
        "    selector = ChiSqSelector(numTopFeatures=5, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
        "    model = selector.fit(df)\n",
        "    selected = model.transform(df)\n",
        "\n",
        "    # --- REVISED: Robustly get feature names from metadata ---\n",
        "    selected_indices = model.selectedFeatures\n",
        "\n",
        "    attrs = metadata[\"ml_attr\"][\"attrs\"]\n",
        "    all_feature_names = []\n",
        "\n",
        "    # The assembler puts OHE (nominal/binary) features first, then numeric features\n",
        "    # We reconstruct the list in that exact order\n",
        "    ohe_features = []\n",
        "    if \"nominal\" in attrs:\n",
        "        ohe_features.extend(attrs[\"nominal\"])\n",
        "    if \"binary\" in attrs: # OHE can also be considered binary\n",
        "        ohe_features.extend(attrs[\"binary\"])\n",
        "\n",
        "    # Sort by \"idx\" to ensure correct order\n",
        "    ohe_features.sort(key=lambda x: x[\"idx\"])\n",
        "    all_feature_names.extend([feat[\"name\"] for feat in ohe_features])\n",
        "\n",
        "    # Add numeric features\n",
        "    numeric_features = attrs.get(\"numeric\", [])\n",
        "    numeric_features.sort(key=lambda x: x[\"idx\"])\n",
        "    all_feature_names.extend([feat[\"name\"] for feat in numeric_features])\n",
        "\n",
        "    # Map the selected indices to their names\n",
        "    selected_feature_names = [all_feature_names[i] for i in selected_indices]\n",
        "    # --- End of revision ---\n",
        "\n",
        "    with open(f\"{output_dir}/task3_feature_selection.txt\", \"w\") as f:\n",
        "        f.write(\"Task 3: Feature Selection using Chi-Square\\n\")\n",
        "        f.write(\"Top 5 features selected (from indices):\\n\")\n",
        "        for i, name in zip(selected_indices, selected_feature_names):\n",
        "            f.write(f\"- Index {i}: {name}\\n\")\n",
        "        f.write(\"\\nSample Output:\\n\")\n",
        "        sample = selected.select(\"selectedFeatures\", \"label\").limit(5).toPandas()\n",
        "        f.write(sample.to_string(index=False))\n",
        "\n",
        "# Task 4: Hyperparameter Tuning and Model Comparison\n",
        "# This function is also generic, but I'll add a small improvement for clarity\n",
        "def tune_and_compare_models(df):\n",
        "    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "    evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol=\"label\")\n",
        "    output_lines = [\"Task 4: Hyperparameter Tuning and Model Comparison\\n\"]\n",
        "\n",
        "    # Define models and parameter grids\n",
        "    lr = LogisticRegression(labelCol=\"label\")\n",
        "    dt = DecisionTreeClassifier(labelCol=\"label\")\n",
        "    rf = RandomForestClassifier(labelCol=\"label\")\n",
        "    gbt = GBTClassifier(labelCol=\"label\")\n",
        "\n",
        "    models = {\n",
        "        \"LogisticRegression\": (lr, ParamGridBuilder()\n",
        "                                .addGrid(lr.regParam, [0.01, 0.1])\n",
        "                                .build()),\n",
        "        \"DecisionTree\": (dt, ParamGridBuilder()\n",
        "                             .addGrid(dt.maxDepth, [5, 10])\n",
        "                             .build()),\n",
        "        \"RandomForest\": (rf, ParamGridBuilder()\n",
        "                             .addGrid(rf.numTrees, [10, 50])\n",
        "                             .build()),\n",
        "        \"GBT\": (gbt, ParamGridBuilder()\n",
        "                   .addGrid(gbt.maxIter, [10, 20])\n",
        "                   .build())\n",
        "    }\n",
        "\n",
        "    for name, (model, grid) in models.items():\n",
        "        output_lines.append(f\"\\nTuning {name}...\")\n",
        "        cv = CrossValidator(estimator=model,\n",
        "                            estimatorParamMaps=grid,\n",
        "                            evaluator=evaluator,\n",
        "                            numFolds=3) # Reduced to 3 folds for faster execution, can be 5\n",
        "\n",
        "        cv_model = cv.fit(train_df)\n",
        "        best_model = cv_model.bestModel\n",
        "        predictions = best_model.transform(test_df)\n",
        "        auc = evaluator.evaluate(predictions)\n",
        "        output_lines.append(f\"{name} Best Model Accuracy (AUC): {auc:.2f}\")\n",
        "\n",
        "        # --- IMPROVEMENT: Print only the tuned parameters ---\n",
        "        best_params_map = best_model.extractParamMap()\n",
        "        tuned_params = {}\n",
        "        for param_map in grid:\n",
        "            for param, value in param_map.items():\n",
        "                tuned_params[param.name] = best_params_map[param]\n",
        "        output_lines.append(f\"Best Params for {name}: {tuned_params}\")\n",
        "        # --- End of improvement ---\n",
        "\n",
        "    with open(f\"{output_dir}/task4_model_comparison.txt\", \"w\") as f:\n",
        "        f.write(\"\\n\".join(output_lines))\n",
        "\n",
        "# --- Run all tasks ---\n",
        "# CHANGED: preprocess_data now returns metadata\n",
        "preprocessed_df, metadata = preprocess_data(df)\n",
        "\n",
        "train_logistic_regression_model(preprocessed_df)\n",
        "\n",
        "# CHANGED: pass metadata to feature_selection\n",
        "feature_selection(preprocessed_df, metadata)\n",
        "\n",
        "tune_and_compare_models(preprocessed_df)\n",
        "\n",
        "# Stop Spark session\n",
        "print(\"MLlib tasks complete. Output files saved to 'outputs' directory.\")\n",
        "spark.stop()"
      ]
    }
  ]
}